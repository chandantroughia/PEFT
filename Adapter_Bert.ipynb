{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f435b68-c725-4bf2-9248-1d445d3e31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import IPython\n",
    "\n",
    "# # Restart the kernel programmatically\n",
    "# IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce450dd-8fa1-4608-a1d2-193dde25e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install adapters transformers datasets evaluate scikit-learn wandb -q\n",
    "!pip install ipywidgets matplotlib -q\n",
    "!pip install accelerate==0.26.0 --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5460f2f2-48ae-4be4-b834-decd4b9175b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed correctly!\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import adapters\n",
    "import wandb\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, set_seed, TrainingArguments, Trainer\n",
    "from adapters import AutoAdapterModel, ADAPTER_CONFIG_MAP\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "import evaluate\n",
    "\n",
    "print(\"‚úÖ All packages installed correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44f85f1-c15d-43a2-840e-d9b61ab8ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the labeled version of PubMedQA (1,000 high-quality samples)\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb0977a-13e8-46e7-86c8-e87f7f170206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 80/20 split since no validation set is provided\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Optional: rename \"test\" split to \"validation\" for compatibility with Hugging Face Trainer\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b452e817-0a1e-4802-a603-102921c07f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05f09b632ac42c3b39756afcd2471dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e470af5aa904aec9248795a70387a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Reformat the dataset for classification\n",
    "def format_for_bert(example):\n",
    "    example[\"text\"] = f\"Question: {example['question']} Context: {example['context']}\"\n",
    "    label_map = {\"yes\": 0, \"no\": 1, \"maybe\": 2}\n",
    "    example[\"label\"] = label_map[example[\"final_decision\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_for_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfecbb72-4b8a-406d-ad46-4c032c05ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision', 'text', 'label'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision', 'text', 'label'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8196f7d-9d6f-41d6-b453-4b998782907f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155998ab4b7d455ba7202b248a9e476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63b1dff4ac4a24902046a62e43ca8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_for_bert(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True  # Ensure segment IDs are included\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_for_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"pubid\", \"question\", \"context\", \"long_answer\", \"final_decision\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650dce39-f02b-4ea6-a4ee-949c347c1d00",
   "metadata": {},
   "source": [
    "## Phase 2\n",
    "\n",
    "‚úÖ Phase 2: Adapter Model Setup\n",
    "\n",
    "üîπ Goal:\n",
    "\n",
    "Load a pre-trained t5-base model and attach a trainable Adapter module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e322851a-c968-4743-be6c-659a590f88ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pfeiffer adapter configuration\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Adapter Model Setup with BERT\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "\n",
    "# Use the Pfeiffer adapter configuration\n",
    "adapter_config_name = \"pfeiffer\"\n",
    "adapter_config = ADAPTER_CONFIG_MAP[adapter_config_name]\n",
    "\n",
    "print(f\"Using {adapter_config_name} adapter configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba7fc92-3308-4637-810f-e0798e495117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandantroughia\u001b[0m (\u001b[33mchandantroughia-cst\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250414_000436-ihxo3rnl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/ihxo3rnl' target=\"_blank\">bert-adapter-20250414-000436</a></strong> to <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/ihxo3rnl' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/ihxo3rnl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb for Adapter approach\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"peft-pubmedqa\",\n",
    "        name=f\"bert-adapter-{time.strftime('%Y%m%d-%H%M%S')}\",\n",
    "        tags=[\"bert\", \"adapter\", \"peft\"],\n",
    "        config={\n",
    "            \"model\": \"bert-base-uncased\",\n",
    "            \"method\": \"adapter\",\n",
    "            \"strategy\": \"peft\",\n",
    "            \"adapter_type\": \"pfeiffer\",\n",
    "            \"dataset\": \"pubmedqa\",\n",
    "            \"seed\": 42\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e62390-49bd-4f48-b9ea-933af6c39290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading BERT model with adapter support...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûï Adding 'pubmedqa_adapter' with pfeiffer configuration to model...\n",
      "üì¶ Total model parameters: 111,592,317\n",
      "üéØ Trainable parameters (adapter only): 2,110,077\n",
      "üí° Parameter efficiency: 1.89%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load BERT with adapter support\n",
    "print(\"üîÑ Loading BERT model with adapter support...\")\n",
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.add_classification_head(\"pubmedqa\", num_labels=3)\n",
    "\n",
    "# Step 4: Attach Adapter\n",
    "print(f\"‚ûï Adding 'pubmedqa_adapter' with {adapter_config_name} configuration to model...\")\n",
    "if \"pubmedqa_adapter\" in model.adapters_config.adapters:\n",
    "    model.delete_adapter(\"pubmedqa_adapter\")\n",
    "model.add_adapter(\"pubmedqa_adapter\", config=adapter_config)\n",
    "model.train_adapter(\"pubmedqa_adapter\")\n",
    "model.set_active_adapters(\"pubmedqa_adapter\")\n",
    "\n",
    "# Parameter stats\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "efficiency = trainable_params / total_params\n",
    "\n",
    "print(f\"üì¶ Total model parameters: {total_params:,}\")\n",
    "print(f\"üéØ Trainable parameters (adapter only): {trainable_params:,}\")\n",
    "print(f\"üí° Parameter efficiency: {efficiency:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d86fda-b87a-4e07-a03f-16518a7cd7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Model moved to cuda\n",
      "‚öôÔ∏è Configuring training parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0382c9f1d854dd5ab7d7e5987d52279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f3d190efc4621beaff0241b03b18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log to wandb\n",
    "wandb.log({\n",
    "    \"total_params\": total_params,\n",
    "    \"trainable_params\": trainable_params,\n",
    "    \"adapter_efficiency\": efficiency\n",
    "})\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"üöÄ Model moved to {device}\")\n",
    "\n",
    "# Step 5: Training Setup - Modified\n",
    "output_dir = f\"./results/bert-adapter-{adapter_config_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuring training parameters...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    "    # Remove metric_for_best_model and load_best_model_at_end \n",
    "    # to simplify the process for now\n",
    "    run_name=f\"bert-adapter-run-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "\n",
    "# Step 6: Define metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e543c38-31b3-4170-b060-13d7ca00f3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1\n\u001b[1;32m     25\u001b[0m     }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Step 7: Train\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m(\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     31\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚è±Ô∏è Starting adapter training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Simplified compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Print some examples for debugging\n",
    "    print(\"\\nSample predictions vs labels:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Prediction: {predictions[i]}, Label: {labels[i]}\")\n",
    "    \n",
    "    # Calculate accuracy manually\n",
    "    correct = (predictions == labels).sum()\n",
    "    total = len(predictions)\n",
    "    accuracy = float(correct) / total\n",
    "    \n",
    "    # Use sklearn for F1\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Step 7: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚è±Ô∏è Starting adapter training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "wandb.log({\"training_time\": training_time})\n",
    "\n",
    "# Save the adapter\n",
    "model.save_adapter(f\"{output_dir}/final_adapter\", \"pubmedqa_adapter\")\n",
    "print(f\"üíæ Adapter saved to {output_dir}/final_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2539b19f-d8ee-4dcb-a4d5-7d1e37304095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating the adapter model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_runtime': 0.9662, 'eval_samples_per_second': 206.991, 'eval_steps_per_second': 25.874, 'epoch': 3.0}\n",
      "\n",
      "Running manual predictions...\n",
      "Accuracy: 0.7900\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.82      0.87      0.84       113\n",
      "          no       0.77      0.70      0.73        67\n",
      "       maybe       0.65      0.65      0.65        20\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.75      0.74      0.74       200\n",
      "weighted avg       0.79      0.79      0.79       200\n",
      "\n",
      "\n",
      "Example predictions:\n",
      "Example 1: Prediction: no, True label: no ‚úì\n",
      "Example 2: Prediction: yes, True label: yes ‚úì\n",
      "Example 3: Prediction: yes, True label: no ‚úó\n",
      "Example 4: Prediction: no, True label: no ‚úì\n",
      "Example 5: Prediction: yes, True label: yes ‚úì\n",
      "Example 6: Prediction: maybe, True label: maybe ‚úì\n",
      "Example 7: Prediction: yes, True label: no ‚úó\n",
      "Example 8: Prediction: yes, True label: yes ‚úì\n",
      "Example 9: Prediction: no, True label: no ‚úì\n",
      "Example 10: Prediction: yes, True label: yes ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"üìä Evaluating the adapter model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Manual prediction and analysis\n",
    "print(\"\\nRunning manual predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Process validation dataset in batches\n",
    "batch_size = 16\n",
    "num_examples = len(tokenized_dataset[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        end_idx = min(i + batch_size, num_examples)\n",
    "        batch_data = tokenized_dataset[\"validation\"][i:end_idx]\n",
    "        \n",
    "        # Convert to appropriate format\n",
    "        input_ids = torch.tensor(batch_data[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch_data[\"attention_mask\"]).to(device)\n",
    "        token_type_ids = torch.tensor(batch_data[\"token_type_ids\"]).to(device)\n",
    "        labels = torch.tensor(batch_data[\"label\"]).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Store results\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert numeric predictions back to text labels\n",
    "label_map_reverse = {0: \"yes\", 1: \"no\", 2: \"maybe\"}\n",
    "pred_texts = [label_map_reverse[pred] for pred in all_preds]\n",
    "label_texts = [label_map_reverse[label] for label in all_labels]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate class-specific metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"yes\", \"no\", \"maybe\"]))\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(min(10, len(pred_texts))):\n",
    "    match = \"‚úì\" if pred_texts[i] == label_texts[i] else \"‚úó\"\n",
    "    print(f\"Example {i+1}: Prediction: {pred_texts[i]}, True label: {label_texts[i]} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbfc96-a1d8-4cf6-b9f9-cf53b9dd0d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
