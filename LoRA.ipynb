{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3639c5c-8ffa-4252-87c1-9b6427251aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Restart the kernel programmatically\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8e3736-ca05-4675-92af-3a803b80eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "  transformers==4.48.0 \\\n",
    "  accelerate==0.26.0 \\\n",
    "  peft==0.7.1 \\\n",
    "  datasets==2.14.5 \\\n",
    "  evaluate==0.4.1 \\\n",
    "  scikit-learn \\\n",
    "  wandb \\\n",
    "  ipywidgets \\\n",
    "  matplotlib \\\n",
    "  --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27be92ef-8564-4f61-b6a8-6d856152f947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment ready for LoRA training!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Optional: Login to wandb (uncomment if not already done)\n",
    "# wandb.login()\n",
    "\n",
    "print(\"✅ Environment ready for LoRA training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6a5f9a-cf6a-4df6-aee4-ebbcabdb282b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07ec89a89914fed81cd4b261cf46771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2720b81092b493ab550cc1fa212a76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7f02fe7b1941e897f0576d7bd6838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c28dfa11544707a350ed9c0a1b2854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20211c5f09a4f1380f970294c162d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e8b3a0ae314220a015032019fa1d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a93896fdab4b40be50c201b9ba6400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset formatted for BERT classification\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled version of PubMedQA\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Create an 80/20 train-validation split\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "\n",
    "# Reformat each example for BERT input\n",
    "def format_for_bert(example):\n",
    "    example[\"text\"] = f\"Question: {example['question']} Context: {example['context']}\"\n",
    "    label_map = {\"yes\": 0, \"no\": 1, \"maybe\": 2}\n",
    "    example[\"label\"] = label_map[example[\"final_decision\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_for_bert)\n",
    "print(\"✅ Dataset formatted for BERT classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e942ff3d-c2c7-4097-8a25-c5469618da69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636ba187815441db78e4701794c06b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde822aa10354d16829edcd4be730ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the input text\n",
    "def tokenize_for_bert(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True  # Important for BERT\n",
    "    )\n",
    "\n",
    "# Apply to both train and validation sets\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_for_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"pubid\", \"question\", \"context\", \"long_answer\", \"final_decision\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7e6e6d-1f9c-4230-bab3-e480f2c1f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandantroughia\u001b[0m (\u001b[33mchandantroughia-cst\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250414_005744-fmqzt4qz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/fmqzt4qz' target=\"_blank\">bert-lora-20250414-005744</a></strong> to <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/fmqzt4qz' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/fmqzt4qz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➕ Applying LoRA...\n",
      "trainable params: 297,219 || all params: 109,781,766 || trainable%: 0.27073621679578375\n",
      "🚀 Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Initialize wandb with LoRA intent\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"peft-pubmedqa\",\n",
    "        name=f\"bert-lora-{time.strftime('%Y%m%d-%H%M%S')}\",\n",
    "        tags=[\"bert\", \"lora\", \"peft\"],\n",
    "        config={\n",
    "            \"model\": \"bert-base-uncased\",\n",
    "            \"method\": \"lora\",  # Indicating the intended method\n",
    "            \"strategy\": \"peft\",\n",
    "            \"lora_rank\": 8,  # These are the intended LoRA parameters\n",
    "            \"lora_alpha\": 16,\n",
    "            \"dataset\": \"pubmedqa\",\n",
    "            \"seed\": 42,\n",
    "            \"note\": \"Implementation uses standard fine-tuning due to library compatibility issues\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Load the base BERT model for classification\n",
    "print(\"🔄 Loading base model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],  # works well with BERT attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "print(\"➕ Applying LoRA...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"🚀 Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0433d164-efcd-4bcc-a8e3-7c328fbe3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./results/bert-lora-finetune\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Metric computation function\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")  # For multiclass\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576fd4f0-2da1-4e51-ace4-94d1d8d34516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.942700</td>\n",
       "      <td>0.975630</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.951400</td>\n",
       "      <td>0.982561</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.989175</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.9354345703125, metrics={'train_runtime': 73.0473, 'train_samples_per_second': 32.855, 'train_steps_per_second': 4.107, 'total_flos': 633663538790400.0, 'train_loss': 0.9354345703125, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # your model should already be moved to GPU (if available)\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"⏱️ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf16b11-217e-4d10-b1a8-3c5fa76550cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating the LoRA model on validation set...\n",
      "\n",
      "✅ Evaluation Results: {'eval_loss': 0.9891747832298279, 'eval_accuracy': 0.515, 'eval_f1': 0.22662266226622663, 'eval_runtime': 1.0456, 'eval_samples_per_second': 191.279, 'eval_steps_per_second': 23.91, 'epoch': 3.0}\n",
      "\n",
      "🔍 Running manual predictions...\n",
      "\n",
      "📈 Manual Accuracy: 0.5150\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.52      1.00      0.68       103\n",
      "          no       0.00      0.00      0.00        72\n",
      "       maybe       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.17      0.33      0.23       200\n",
      "weighted avg       0.27      0.52      0.35       200\n",
      "\n",
      "\n",
      "🔎 Sample Predictions:\n",
      "Example 1: Prediction: yes, True label: yes ✓\n",
      "Example 2: Prediction: yes, True label: no ✗\n",
      "Example 3: Prediction: yes, True label: yes ✓\n",
      "Example 4: Prediction: yes, True label: maybe ✗\n",
      "Example 5: Prediction: yes, True label: maybe ✗\n",
      "Example 6: Prediction: yes, True label: no ✗\n",
      "Example 7: Prediction: yes, True label: maybe ✗\n",
      "Example 8: Prediction: yes, True label: yes ✓\n",
      "Example 9: Prediction: yes, True label: maybe ✗\n",
      "Example 10: Prediction: yes, True label: no ✗\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Step 1: Evaluate using built-in evaluate()\n",
    "print(\"📊 Evaluating the LoRA model on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\n✅ Evaluation Results: {eval_results}\")\n",
    "\n",
    "# ✅ Step 2: Manual Predictions\n",
    "print(\"\\n🔍 Running manual predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Process validation dataset in batches\n",
    "batch_size = 16\n",
    "num_examples = len(tokenized_dataset[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        end_idx = min(i + batch_size, num_examples)\n",
    "        batch_data = tokenized_dataset[\"validation\"][i:end_idx]\n",
    "        \n",
    "        # Convert to appropriate format\n",
    "        input_ids = torch.tensor(batch_data[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch_data[\"attention_mask\"]).to(device)\n",
    "        token_type_ids = torch.tensor(batch_data[\"token_type_ids\"]).to(device)\n",
    "        labels = torch.tensor(batch_data[\"label\"]).to(device)  # Note: using \"label\" (singular)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Store results\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ✅ Step 3: Decode numeric predictions to labels\n",
    "label_map_reverse = {0: \"yes\", 1: \"no\", 2: \"maybe\"}\n",
    "pred_texts = [label_map_reverse[p] for p in all_preds]\n",
    "label_texts = [label_map_reverse[l] for l in all_labels]\n",
    "\n",
    "# ✅ Step 4: Metrics\n",
    "accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_preds)\n",
    "print(f\"\\n📈 Manual Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n📋 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"yes\", \"no\", \"maybe\"]))\n",
    "\n",
    "print(\"\\n🔎 Sample Predictions:\")\n",
    "for i in range(min(10, len(pred_texts))):\n",
    "    match = \"✓\" if pred_texts[i] == label_texts[i] else \"✗\"\n",
    "    print(f\"Example {i+1}: Prediction: {pred_texts[i]}, True label: {label_texts[i]} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b1c2e-6569-4f79-b061-4f0411fbedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
