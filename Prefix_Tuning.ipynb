{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1185f-c851-44ac-add8-799ac6e50a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Restart the kernel to ensure a clean environment\n",
    "import os\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb50ad9-e756-411a-927e-78ba5e457787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.48.0 accelerate==0.26.0 peft==0.7.1 datasets==2.14.5 evaluate==0.4.1 scikit-learn wandb ipywidgets matplotlib --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e09b752-f5fd-45d0-a579-58807b68ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready for Prefix Tuning!\n",
      "‚úÖ Dataset formatted for BERT classification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00ea8897f6d4000b95abe9c6af93590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f512fe0789947de8d8d7ceaf12643c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandantroughia\u001b[0m (\u001b[33mchandantroughia-cst\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250414_032414-t7wdo3re</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/t7wdo3re' target=\"_blank\">bert-prefix-tuning-20250414-032413</a></strong> to <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/t7wdo3re' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/t7wdo3re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ûï Applying Prefix Tuning...\n",
      "trainable params: 14,782,467 || all params: 124,267,014 || trainable%: 11.895728821487575\n",
      "üöÄ Model moved to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1984/3522386742.py:133: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.972476</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.950400</td>\n",
       "      <td>0.980366</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.982600</td>\n",
       "      <td>0.988265</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating the Prefix Tuning model on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation Results: {'eval_loss': 0.9882653951644897, 'eval_accuracy': 0.515, 'eval_f1': 0.22662266226622663, 'eval_runtime': 0.899, 'eval_samples_per_second': 222.473, 'eval_steps_per_second': 27.809, 'epoch': 3.0}\n",
      "\n",
      "üîç Running manual predictions...\n",
      "\n",
      "üìà Manual Accuracy: 0.5150\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.52      1.00      0.68       103\n",
      "          no       0.00      0.00      0.00        72\n",
      "       maybe       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.17      0.33      0.23       200\n",
      "weighted avg       0.27      0.52      0.35       200\n",
      "\n",
      "\n",
      "üîé Sample Predictions:\n",
      "Example 1: Prediction: yes, True label: yes ‚úì\n",
      "Example 2: Prediction: yes, True label: no ‚úó\n",
      "Example 3: Prediction: yes, True label: yes ‚úì\n",
      "Example 4: Prediction: yes, True label: maybe ‚úó\n",
      "Example 5: Prediction: yes, True label: maybe ‚úó\n",
      "Example 6: Prediction: yes, True label: no ‚úó\n",
      "Example 7: Prediction: yes, True label: maybe ‚úó\n",
      "Example 8: Prediction: yes, True label: yes ‚úì\n",
      "Example 9: Prediction: yes, True label: maybe ‚úó\n",
      "Example 10: Prediction: yes, True label: no ‚úó\n",
      "üíæ Prefix Tuning model saved to ./results/bert-prefix-tuning/final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for Prefix Tuning!\")\n",
    "\n",
    "# Load the labeled version of PubMedQA\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Create an 80/20 train-validation split\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "\n",
    "# Reformat each example for BERT input\n",
    "def format_for_bert(example):\n",
    "    example[\"text\"] = f\"Question: {example['question']} Context: {example['context']}\"\n",
    "    label_map = {\"yes\": 0, \"no\": 1, \"maybe\": 2}\n",
    "    example[\"label\"] = label_map[example[\"final_decision\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_for_bert)\n",
    "print(\"‚úÖ Dataset formatted for BERT classification\")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the input text - with reduced max_length to make room for prefix tokens\n",
    "def tokenize_for_bert(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=492,  # Reduced to make room for prefix tokens (512 - 20 = 492)\n",
    "        return_token_type_ids=True  # Important for BERT\n",
    "    )\n",
    "\n",
    "# Apply to both train and validation sets\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_for_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"pubid\", \"question\", \"context\", \"long_answer\", \"final_decision\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete\")\n",
    "\n",
    "# Set up Prefix Tuning\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Initialize wandb\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"peft-pubmedqa\",\n",
    "        name=f\"bert-prefix-tuning-{time.strftime('%Y%m%d-%H%M%S')}\",\n",
    "        tags=[\"bert\", \"prefix-tuning\", \"peft\"],\n",
    "        config={\n",
    "            \"model\": \"bert-base-uncased\",\n",
    "            \"method\": \"prefix-tuning\",\n",
    "            \"strategy\": \"peft\",\n",
    "            \"prefix_length\": 20,\n",
    "            \"dataset\": \"pubmedqa\",\n",
    "            \"seed\": 42,\n",
    "            \"max_input_length\": 492  # Note the reduced input length\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Load the base BERT model for classification\n",
    "print(\"üîÑ Loading base model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define Prefix Tuning configuration\n",
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    num_virtual_tokens=20,  # Length of prefix\n",
    "    prefix_projection=True,  # Use MLP for prefix projection\n",
    "    encoder_hidden_size=768  # Hidden size for BERT\n",
    ")\n",
    "\n",
    "# Wrap the model with Prefix Tuning\n",
    "print(\"‚ûï Applying Prefix Tuning...\")\n",
    "model = get_peft_model(model, prefix_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"üöÄ Model moved to {device}\")\n",
    "\n",
    "# Set up training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./results/bert-prefix-tuning\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    "    run_name=f\"bert-prefix-run-{time.strftime('%Y%m%d-%H%M%S')}\"  # Added unique run_name\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Metric computation function\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")  # For multiclass\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"‚è±Ô∏è Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"üìä Evaluating the Prefix Tuning model on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\n‚úÖ Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Manual prediction and detailed analysis\n",
    "print(\"\\nüîç Running manual predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Process validation dataset in batches\n",
    "batch_size = 16\n",
    "num_examples = len(tokenized_dataset[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        end_idx = min(i + batch_size, num_examples)\n",
    "        batch_data = tokenized_dataset[\"validation\"][i:end_idx]\n",
    "        \n",
    "        # Convert to appropriate format\n",
    "        input_ids = torch.tensor(batch_data[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch_data[\"attention_mask\"]).to(device)\n",
    "        token_type_ids = torch.tensor(batch_data[\"token_type_ids\"]).to(device)\n",
    "        labels = torch.tensor(batch_data[\"label\"]).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Store results\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert numeric predictions back to text labels\n",
    "label_map_reverse = {0: \"yes\", 1: \"no\", 2: \"maybe\"}\n",
    "pred_texts = [label_map_reverse[p] for p in all_preds]\n",
    "label_texts = [label_map_reverse[l] for l in all_labels]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_preds)\n",
    "print(f\"\\nüìà Manual Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate class-specific metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"yes\", \"no\", \"maybe\"]))\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüîé Sample Predictions:\")\n",
    "for i in range(min(10, len(pred_texts))):\n",
    "    match = \"‚úì\" if pred_texts[i] == label_texts[i] else \"‚úó\"\n",
    "    print(f\"Example {i+1}: Prediction: {pred_texts[i]}, True label: {label_texts[i]} {match}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "print(f\"üíæ Prefix Tuning model saved to {output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70114ddd-e210-445f-bbc0-9729de867a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
