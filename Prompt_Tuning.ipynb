{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29880396-43e2-473a-b44c-f51589edfb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Restart the kernel to ensure a clean environment\n",
    "import os\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbe4f1f-892f-45c7-8c58-6b554d8fc662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready for Prompt Tuning!\n",
      "‚úÖ Dataset formatted for BERT classification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e300c770543179d97d7889243dc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandantroughia\u001b[0m (\u001b[33mchandantroughia-cst\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250414_031833-xhuboect</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/xhuboect' target=\"_blank\">bert-prompt-tuning-20250414-031833</a></strong> to <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/xhuboect' target=\"_blank\">https://wandb.ai/chandantroughia-cst/peft-pubmedqa/runs/xhuboect</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading base model...\n",
      "‚ûï Applying Prompt Tuning...\n",
      "trainable params: 9,987 || all params: 109,494,534 || trainable%: 0.009121003245696264\n",
      "üöÄ Model moved to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1702/3855318639.py:132: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.961600</td>\n",
       "      <td>0.978425</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.977217</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>0.980354</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>0.980540</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.944500</td>\n",
       "      <td>0.978910</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>0.979912</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.921100</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.979384</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.925800</td>\n",
       "      <td>0.979985</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.226623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating the Prompt Tuning model on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation Results: {'eval_loss': 0.9799853563308716, 'eval_accuracy': 0.515, 'eval_f1': 0.22662266226622663, 'eval_runtime': 0.8989, 'eval_samples_per_second': 222.482, 'eval_steps_per_second': 27.81, 'epoch': 10.0}\n",
      "\n",
      "üîç Running manual predictions...\n",
      "\n",
      "üìà Manual Accuracy: 0.5150\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.52      1.00      0.68       103\n",
      "          no       0.00      0.00      0.00        72\n",
      "       maybe       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.52       200\n",
      "   macro avg       0.17      0.33      0.23       200\n",
      "weighted avg       0.27      0.52      0.35       200\n",
      "\n",
      "\n",
      "üîé Sample Predictions:\n",
      "Example 1: Prediction: yes, True label: yes ‚úì\n",
      "Example 2: Prediction: yes, True label: no ‚úó\n",
      "Example 3: Prediction: yes, True label: yes ‚úì\n",
      "Example 4: Prediction: yes, True label: maybe ‚úó\n",
      "Example 5: Prediction: yes, True label: maybe ‚úó\n",
      "Example 6: Prediction: yes, True label: no ‚úó\n",
      "Example 7: Prediction: yes, True label: maybe ‚úó\n",
      "Example 8: Prediction: yes, True label: yes ‚úì\n",
      "Example 9: Prediction: yes, True label: maybe ‚úó\n",
      "Example 10: Prediction: yes, True label: no ‚úó\n",
      "üíæ Prompt Tuning model saved to ./results/bert-prompt-tuning/final_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for Prompt Tuning!\")\n",
    "\n",
    "# Load the labeled version of PubMedQA\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Create an 80/20 train-validation split\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "\n",
    "# Reformat each example for BERT input\n",
    "def format_for_bert(example):\n",
    "    example[\"text\"] = f\"Question: {example['question']} Context: {example['context']}\"\n",
    "    label_map = {\"yes\": 0, \"no\": 1, \"maybe\": 2}\n",
    "    example[\"label\"] = label_map[example[\"final_decision\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_for_bert)\n",
    "print(\"‚úÖ Dataset formatted for BERT classification\")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the input text\n",
    "def tokenize_for_bert(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=502,\n",
    "        return_token_type_ids=True  # Important for BERT\n",
    "    )\n",
    "\n",
    "# Apply to both train and validation sets\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_for_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"pubid\", \"question\", \"context\", \"long_answer\", \"final_decision\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete\")\n",
    "\n",
    "# Set up Prompt Tuning\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Initialize wandb\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"peft-pubmedqa\",\n",
    "        name=f\"bert-prompt-tuning-{time.strftime('%Y%m%d-%H%M%S')}\",\n",
    "        tags=[\"bert\", \"prompt-tuning\", \"peft\"],\n",
    "        config={\n",
    "            \"model\": \"bert-base-uncased\",\n",
    "            \"method\": \"prompt-tuning\",\n",
    "            \"strategy\": \"peft\",\n",
    "            \"prompt_length\": 10,\n",
    "            \"dataset\": \"pubmedqa\",\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Load the base BERT model for classification\n",
    "print(\"üîÑ Loading base model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define Prompt Tuning configuration\n",
    "prompt_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    num_virtual_tokens=10,  # Number of virtual tokens to add\n",
    "    tokenizer_name_or_path=\"bert-base-uncased\",\n",
    "    prompt_tuning_init=\"RANDOM\"  # Initialize randomly\n",
    ")\n",
    "\n",
    "# Wrap the model with Prompt Tuning\n",
    "print(\"‚ûï Applying Prompt Tuning...\")\n",
    "model = get_peft_model(model, prompt_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"üöÄ Model moved to {device}\")\n",
    "\n",
    "# Set up training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./results/bert-prompt-tuning\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    "    run_name=f\"bert-prompt-run-{time.strftime('%Y%m%d-%H%M%S')}\"  # Added unique run_name\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Metric computation function\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")  # For multiclass\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"‚è±Ô∏è Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"üìä Evaluating the Prompt Tuning model on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\n‚úÖ Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Manual prediction and detailed analysis\n",
    "print(\"\\nüîç Running manual predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Process validation dataset in batches\n",
    "batch_size = 16\n",
    "num_examples = len(tokenized_dataset[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        end_idx = min(i + batch_size, num_examples)\n",
    "        batch_data = tokenized_dataset[\"validation\"][i:end_idx]\n",
    "        \n",
    "        # Convert to appropriate format\n",
    "        input_ids = torch.tensor(batch_data[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch_data[\"attention_mask\"]).to(device)\n",
    "        token_type_ids = torch.tensor(batch_data[\"token_type_ids\"]).to(device)\n",
    "        labels = torch.tensor(batch_data[\"label\"]).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Store results\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert numeric predictions back to text labels\n",
    "label_map_reverse = {0: \"yes\", 1: \"no\", 2: \"maybe\"}\n",
    "pred_texts = [label_map_reverse[p] for p in all_preds]\n",
    "label_texts = [label_map_reverse[l] for l in all_labels]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_preds)\n",
    "print(f\"\\nüìà Manual Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate class-specific metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"yes\", \"no\", \"maybe\"]))\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüîé Sample Predictions:\")\n",
    "for i in range(min(10, len(pred_texts))):\n",
    "    match = \"‚úì\" if pred_texts[i] == label_texts[i] else \"‚úó\"\n",
    "    print(f\"Example {i+1}: Prediction: {pred_texts[i]}, True label: {label_texts[i]} {match}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "print(f\"üíæ Prompt Tuning model saved to {output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b0c51-25a2-41a8-ac09-4da8de06779b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
